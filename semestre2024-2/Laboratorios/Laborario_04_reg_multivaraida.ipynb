{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttsES2kOOUaz"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2024-2/Laboratorios/Laboratorio_04_reg_multivariada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRIfl4W2cLM"
      },
      "source": [
        "# Regresion multivariada\n",
        "\n",
        "Supongamos que tenemos un conjunto de caracteristicas $X = X_1,X_2...X_j...X_n$ para realizar una  predicción $y$ con valores esperados $\\hat{y}$.  \n",
        "\n",
        "Cada X, puede ser escrito como:\n",
        " $X_1 = x_1^{(1)},x_1^{(2)}, x_1^{(3)}...x_1^{(m)}$,\n",
        "\n",
        " $X_2 = x_2^{(1)},x_2^{(2)}, x_2^{(3)}...x_2^{(m)}$,\n",
        "\n",
        " .\n",
        "\n",
        " .\n",
        "\n",
        " .\n",
        "\n",
        " $X_n = x_n^{(1)},x_n^{(2)}, x_n^{(3)}...x_n^{(m)}$.\n",
        "\n",
        "\n",
        "Siendo n el número de caracteristicas y m el número de datos de datos,\n",
        "$\\hat{y} = \\hat{y}_1^{(1)}, \\hat{y}_1^{(2)}...\\hat{y}_1^{(m)} $, el conjunto de datos etiquetados  y $y = y_1^{(1)}, y_1^{(2)}...y_1^{(m)} $ los valores predichos por un modelo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lo anterior puede ser resumido  como:\n",
        "\n",
        "\n",
        "\n",
        "|Training|$\\hat{y}$      | X_1  | X_2  |  .  | .|. |. | X_n|\n",
        "|--------|-------|------|------|-----|--|--|--|----|\n",
        "|1|$\\hat{y}_1^{1}$ | $x_1^{1}$|$x_2^{1}$| .  | .|. |. | $x_n^{1}$|\n",
        "|2|$\\hat{y}_1^{2}$ | $x_1^{2}$|$x_2^{2}$| .  | .|. |. | $x_n^{2}$|\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|m|$\\hat{y}_1^{m}$ | $x_1^{m}$  |$x_2^{m}$| .  | .|. |. | $x_n^{m}$|\n",
        "\n",
        "\n",
        "y el el modelo puede ser ajustado como sigue:\n",
        "\n",
        "Para un solo conjunto de datos de entrenamiento tentemos que:\n",
        "\n",
        "$y = h(\\theta_0,\\theta_1,\\theta_2,...,\\theta_n ) = \\theta_0 + \\theta_1 x_1+\\theta_2 x_2 + \\theta_3 x_3 +...+ \\theta_n x_n $.\n",
        "\n",
        "\\begin{equation}\n",
        "h_{\\Theta}(x) = [\\theta_0,\\theta_1,...,\\theta_n ]\\begin{bmatrix}\n",
        "1^{(1)}\\\\\n",
        "x_1^{(1)}\\\\\n",
        "x_2^{(1)}\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "x_n^{(1)}\\\\\n",
        "\\end{bmatrix} = \\Theta^T X^{(1)}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Para todo el conjunto de datos, tenemos que:\n",
        "\n",
        "Sea $\\Theta^T = [\\theta_0,\\theta_1,\\theta_2,...,\\theta_n]$ una matrix $1 \\times (n+1)$ y  \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "1& 1 & 1 & .&.&.&1\\\\\n",
        "x_1^{(1)}&x_1^{(2)} & x_1^{(3)} & .&.&.&x_1^{(m)}\\\\\n",
        ".&. & . &.&.&.& .\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        "x_n^{(1)}&x_n^{(2)} & x^{(3)} & .&.&.&x_n^{(m)}\\\\\n",
        "\\end{bmatrix}_{(n+1) \\times m}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "luego $h = \\Theta^{T} X $ con dimension $1\\times m$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "La anterior ecuación, es un hiperplano en $\\mathbb{R}^n$. Notese que en caso de tener una sola característica, la ecuación puede ser análizada según lo visto en la sesión de regresion lineal.\n",
        "\n",
        "\n",
        "Para la optimización, vamos a definir la función de coste **$J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )$** , como la función  asociada a la minima distancia entre dos puntos, según la metrica euclidiana.\n",
        "\n",
        "- Metrica Eculidiana\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )=\\frac{1}{2m} \\sum_{i=1}^m ( h_{\\Theta} (X)-\\hat{y}^{(i)})^2 =\\frac{1}{2m} \\sum_{i = 1}^m (\\Theta^{T} X - \\hat{y}^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "Otras métricas pueden ser definidas como sigue en la siguiente referencia.  [Metricas](https://jmlb.github.io/flashcards/2018/04/21/list_cost_functions_fo_neuralnets/).\n",
        "\n",
        "Nuestro objetivo será encontrar los valores mínimos\n",
        "$\\Theta = \\theta_0,\\theta_1,\\theta_2,...,\\theta_n$ que minimizan el error, respecto a los valores etiquetados y esperados $\\hat{y}$\n",
        "\n",
        "\n",
        "Para encontrar $\\Theta$ opmitimo, se necesita  minimizar la función de coste, que permite obtener los valores más cercanos,  esta minimización podrá ser realizada a través de diferentes metodos, el más conocido es el gradiente descendente.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ApulK23HfL"
      },
      "source": [
        "\n",
        "## Gradiente descendente\n",
        "\n",
        "Consideremos la función de coste sin realizar el promedio  de funcion de coste:\n",
        "\\begin{equation}\n",
        "\\Lambda^T =\n",
        "\\begin{bmatrix}\n",
        "(\\theta_0 1 + \\theta_1 x_1^1+\\theta_2 x_2^2 + \\theta_3 x_3^3 +...+ \\theta_n x_n^n - \\hat{y}^{1})^2 \\\\\n",
        "(\\theta_0 1+ \\theta_1 x_1^1+\\theta_2 x_2^2 + \\theta_3 x_3^3 +...+ \\theta_n x_n^n - \\hat{y}^{2})^2\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "(\\theta_0 1 + \\theta_1 x_1^m+\\theta_2 x_2^m + \\theta_3 x_3^m +...+ \\theta_n x_n^m - \\hat{y}^{m})^2\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "$\\Lambda= [\\Lambda_1,\\Lambda_2, ...,\\Lambda_m]$\n",
        "\n",
        "$J = \\frac{1}{2m} \\sum_{i}^m \\Lambda_i $\n",
        "\n",
        "El gradiente descente, puede ser escrito como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\vec{\\Theta} =  - \\alpha \\nabla J(\\theta_0, \\theta_1,...,\\theta_n)\n",
        "\\end{equation}\n",
        "\n",
        "escogiendo el valor j-esimo tenemos que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j}\n",
        "\\end{equation}\n",
        "\n",
        "Aplicando lo anterior a a funcion de coste asociada a la metrica ecuclidiana, tenemos que:\n",
        "\n",
        "Para $j = 0$,\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_0 :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_0} = \\frac{1}{m}\\alpha \\sum_{i=1}^m (\\theta_j X_{ji} - \\hat{y}^{(i)}) 1\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Para $0<j<n $\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j} = \\frac{1}{m} \\alpha\\sum_{i=1}^m (\\theta_{j} X_{ji} - \\hat{y}^{(i)}) X_j\n",
        "\\end{equation}\n",
        "\n",
        "donde X_j es el vector de entrenamiento j-esimo.\n",
        "\n",
        "Lo  anterior puede ser generalizado como siguem, teniendo presente que $X_0 = \\vec{1}$\n",
        "\n",
        "\n",
        "Para $0\\leq j<n$,\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j} = \\frac{1}{m} \\alpha\\sum_{i=1}^m (\\theta_j X_{ji} - \\hat{y}^{(i)}) X_j\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "# Vectorizando el grandiente descendete, tenemos que:\n",
        "\\begin{equation}\n",
        "\\nabla J = \\Lambda^T X\n",
        "\\end{equation}\n",
        "\n",
        "Luego:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Theta=\\Theta-\\alpha \\nabla J\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eavd6K24VDP"
      },
      "source": [
        "# Laboratorio 05\n",
        "Objetivo: Programar una regresión multivariada\n",
        "\n",
        "\n",
        "1. Para simular un conjunto de características $x_1$ , $x_2$,..., $x_n$ trabajaremos en la primera parte con dos características de datos aleatorios que presentan un plano y mostraremos que los párametros optimizados se corresponden con el valor esperado.\n",
        "\n",
        "- Definir la ecuación  $y = 2.1*x_1 - 3.1*x_2$, y generar números aleatorios que pertenecen al plano.\n",
        "\n",
        "- Realizar un diagrama 3D de los puntos generados aleatoriamente.\n",
        "\n",
        "\n",
        "Nuestro objetivo será encontrar los valores $\\theta_0 = 0, \\theta_1=2.1, \\theta_1=3.1$ que mejor ajustar el plano, empleando cálculos vectorizados.\n",
        "\n",
        "2. Inicializar conjunto de parámetros $\\Theta$ de manera aleatoria.\n",
        "3. Construir la matrix X con dimensiones $(n+1, m)$, m es el numero de datos de entrenamiento y (n) el número de caracteristicas.\n",
        "4. Calcular la función de coste(revise cuidosamente las dimensiones de cada matriz):\n",
        "\n",
        "  - $h = \\Theta^{T} X $\n",
        "  - $\\Lambda= (h -Y) $\n",
        "  - $\\Lambda*= (h -Y)^2 $\n",
        "  - $\\Lambda= [\\Lambda_1,\\Lambda_2, ...,\\Lambda_m]$\n",
        "  - $J = \\frac{1}{2m} \\sum_{i}^m \\Lambda_i $\n",
        "\n",
        "5. Aplicar el gradiente descendente:\n",
        "  - Encontrar el gradiente.\n",
        "    $\\nabla J$ = \\Lambda X.T\n",
        "  \n",
        "  - Actualizar los nuevos parametros:\n",
        "    $\\Theta_{n+1}=\\Theta_{n}-\\alpha\\nabla J$\n",
        "\n",
        "\n",
        "6. Iterar para encontrar los valores $\\Theta$ que se ajustan el plano.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OriGxD33w_f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Laborario_04_reg_multivaraida.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}