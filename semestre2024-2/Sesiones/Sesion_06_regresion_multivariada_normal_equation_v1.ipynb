{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGsdu3aXY5O_"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2024-2/Sesiones/Sesion_06_regresion_multivariada_normal_equation_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnE9vnCkY2FK"
      },
      "source": [
        "# Regresion multivariada ecuacion normal, repaso breve.\n",
        "\n",
        "Supongamos que tenemos un conjunto de caracteristicas $X = X_1,X_2...X_j...X_n$ para realizar una  predicción $y$ con valores esperados $\\hat{y}$.  \n",
        "\n",
        "Cada X, puede ser escrito como:\n",
        " $X_1 = x_1^{(1)},x_1^{(2)}, x_1^{(3)}...x_1^{(m)}$,\n",
        "\n",
        " $X_2 = x_2^{(1)},x_2^{(2)}, x_2^{(3)}...x_2^{(m)}$,\n",
        "\n",
        " .\n",
        "\n",
        " .\n",
        "\n",
        " .\n",
        "\n",
        " $X_n = x_n^{(1)},x_n^{(2)}, x_n^{(3)}...x_n^{(m)}$.\n",
        "\n",
        "\n",
        "Siendo n el número de caracteristicas y m el número de datos de datos,\n",
        "$\\hat{y} = \\hat{y}_1^{(1)}, \\hat{y}_1^{(2)}...\\hat{y}_1^{(m)} $, el conjunto de datos etiquetados  y $y = y_1^{(1)}, y_1^{(2)}...y_1^{(m)} $ los valores predichos por un modelo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lo anterior puede ser resumido  como:\n",
        "\n",
        "\n",
        "\n",
        "|Training|$\\hat{y}$      | X_1  | X_2  |  .  | .|. |. | X_n|\n",
        "|--------|-------|------|------|-----|--|--|--|----|\n",
        "|1|$\\hat{y}_1^{1}$ | $x_1^{1}$|$x_2^{1}$| .  | .|. |. | $x_n^{1}$|\n",
        "|2|$\\hat{y}_1^{2}$ | $x_1^{2}$|$x_2^{2}$| .  | .|. |. | $x_n^{2}$|\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|.|.         | .        |.| .  | .|. |. | |\n",
        "|m|$\\hat{y}_1^{m}$ | $x_1^{m}$  |$x_2^{m}$| .  | .|. |. | $x_n^{m}$|\n",
        "\n",
        "\n",
        "y el el modelo puede ser ajustado como sigue:\n",
        "\n",
        "Para un solo conjunto de datos de entrenamiento tentemos que:\n",
        "\n",
        "$y = h(\\theta_0,\\theta_1,\\theta_2,...,\\theta_n ) = \\theta_0 + \\theta_1 x_1+\\theta_2 x_2 + \\theta_3 x_3 +...+ \\theta_n x_n $.\n",
        "\n",
        "\\begin{equation}\n",
        "h_{\\Theta}(x) = [\\theta_0,\\theta_1,...,\\theta_n ]\\begin{bmatrix}\n",
        "1^{(1)}\\\\\n",
        "x_1^{(1)}\\\\\n",
        "x_2^{(1)}\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "x_n^{(1)}\\\\\n",
        "\\end{bmatrix} = \\Theta^T X^{(1)}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Para todo el conjunto de datos, tenemos que:\n",
        "\n",
        "Sea $\\Theta^T = [\\theta_0,\\theta_1,\\theta_2,...,\\theta_n]$ una matrix $1 \\times (n+1)$ y  \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "1& 1 & 1 & .&.&.&1\\\\\n",
        "x_1^{(1)}&x_1^{(2)} & x_1^{(3)} & .&.&.&x_1^{(m)}\\\\\n",
        ".&. & . &.&.&.& .\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        "x_n^{(1)}&x_n^{(2)} & x^{(3)} & .&.&.&x_n^{(m)}\\\\\n",
        "\\end{bmatrix}_{(n+1) \\times m}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "luego $h = \\Theta^{T} X $ con dimension $1\\times m$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "La anterior ecuación, es un hiperplano en $\\mathbb{R}^n$. Notese que en caso de tener una sola característica, la ecuación puede ser análizada según lo visto en la sesión de regresion lineal.\n",
        "\n",
        "\n",
        "Para la optimización, vamos a definir la función de coste **$J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )$** , como la función  asociada a la minima distancia entre dos puntos, según la metrica euclidiana.\n",
        "\n",
        "- Metrica Eculidiana\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )=\\frac{1}{2m} \\sum_{i=1}^m ( h_{\\Theta} (X)-\\hat{y}^{(i)})^2 =\\frac{1}{2m} \\sum_{i = 1}^m (\\Theta^{T} X - \\hat{y}^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "Otras métricas pueden ser definidas como sigue en la siguiente referencia.  [Metricas](https://jmlb.github.io/flashcards/2018/04/21/list_cost_functions_fo_neuralnets/).\n",
        "\n",
        "Nuestro objetivo será encontrar los valores mínimos\n",
        "$\\Theta = \\theta_0,\\theta_1,\\theta_2,...,\\theta_n$ que minimizan el error, respecto a los valores etiquetados y esperados $\\hat{y}$\n",
        "\n",
        "\n",
        "Para encontrar $\\Theta$ optimo, se necesita  minimizar la función de coste. Ecnontremos los valores exactos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT4-yW7jZTmz"
      },
      "source": [
        "# Normal equation\n",
        "Se puede encontrar una solucion exacta para theta sin necesidad de emplear el gradiente descente de la sesiones pasadas, para ellos se puede encontrar el valor minimo de theta y a partir de alli determinar el valor de theta que minimiza J.\n",
        "\n",
        "Los pasos para esta minimizacion se dejan como tarea, y pueden ser calculados según lo siguiente:\n",
        "\n",
        "Si J es la funcion de coste dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )=\\frac{1}{2m} \\sum_{i = 1}^m (\\Theta^{T} X - \\hat{y}^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Demostrar que:\n",
        "\n",
        "- $J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n ) = \\frac{1}{2m} (\\Theta ^ T X - y)^T (\\Theta ^ T X - y)$\n",
        "\n",
        "- $ \\nabla _{\\theta} J = \\frac{1}{m}( (X^T X) \\Theta - X^T y)$\n",
        "\n",
        "\n",
        "Para encontrar el valor minimo de \\theta,  $\\nabla _{\\theta} J = 0$,\n",
        "\n",
        "- $\\Theta = (X^T X)^{-1} X^T y$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Para la demostracion anterior emplee las siguientes propiedades:\n",
        "\n",
        "- $z^T z= \\sum_i z_i^2$\n",
        "- $a^T b = b^Ta$\n",
        "- $\\nabla _x b^T x = b$\n",
        "- $\\nabla _x  x^T A x = 2Ax$\n",
        "\n",
        "donde a, b, x son matrices, $\\nabla_x$ es la derivada respecto al vector x, y A es una matriz simétrica\n",
        "\n",
        "\n",
        "# Demostracion\n",
        "\n",
        "sea $X \\in R^{n\\times m}$ ,  $X^T \\in R^{m\\times n}$\n",
        "\n",
        "sea $Y \\in R^{m \\times 1}$,  $Y^T \\in R^{1 \\times m}$\n",
        "\n",
        "sea $\\Theta \\in R^{n \\times 1}$, $\\Theta^T \\in R^{1 \\times n}$\n",
        "\n",
        "Luego la función de coste puede ser escrita como:\n",
        "\n",
        "$J= (\\Theta ^T X - Y^T)(\\Theta ^T X - Y^T)^T$\n",
        "\n",
        "$J= (\\Theta ^T X) (\\Theta ^T X)^T - (\\Theta ^T X) Y - Y^T (\\Theta ^T X)^T+ Y^TY $\n",
        "\n",
        "\n",
        "$J= (\\Theta ^T X) (\\Theta ^T X)^T - (Y^T(\\Theta ^T X)^T)^T  - Y^T (\\Theta ^T X)^T+ Y^TY $\n",
        "\n",
        "\n",
        "\n",
        "$J= (\\Theta ^T X) (\\Theta ^T X)^T - 2(\\Theta ^T X)Y  + Y^TY $\n",
        "\n",
        "$\\frac{d J}{d\\Theta} = X(\\Theta^T X)^T + (\\Theta ^T X) X^T-2XY$\n",
        "\n",
        "$\\frac{d J}{d\\Theta} = 2 X(\\Theta^T X)^T -2XY$\n",
        "\n",
        "$\\frac{d J}{d\\Theta} = 2 X(X^T \\Theta) -2XY$\n",
        "\n",
        "$2 X(X^T \\Theta) -2XY = 0$\n",
        "\n",
        "$(X X^T)\\Theta = XY$\n",
        "\n",
        "$(X X^T)(X X^T)^{-1}\\Theta = (X X^T)^{-1}XY$\n",
        "\n",
        "$\\Theta = (X X^T)^{-1}XY$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9W1oojJTv89P"
      },
      "outputs": [],
      "source": [
        "#from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "#from matplotlib.ticker import LinearLocator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1oyJaDpCv5aq"
      },
      "outputs": [],
      "source": [
        "# Regresion lineal  simple\n",
        "N = 10\n",
        "x1 = np.linspace(-1, 1, N)\n",
        "y = 2*x1 #- 3*x2 + 0.0\n",
        "df = pd.DataFrame({\"Y\":y, \"X1\":x1})\n",
        "df[\"ones\"] = np.ones(N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "RVUm-SbqFb9N",
        "outputId": "df67b20a-e1c9-4339-e5c8-e15af528a0ee"
      },
      "outputs": [],
      "source": [
        "plt.plot(df.X1,df.Y,\"ro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SFXNDeWnb1IV"
      },
      "outputs": [],
      "source": [
        "#np.shape(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7iagdu1z2Xz",
        "outputId": "b7d74106-405d-4576-95e7-ff5af06cddaf"
      },
      "outputs": [],
      "source": [
        "y = np.reshape(df.Y.values, (N,1))\n",
        "\n",
        "X = df[[\"ones\",\"X1\"]].values\n",
        "X = np.matrix(X)\n",
        "theta = (X.T@X).I @ X.T @ y\n",
        "theta = np.array(theta).flatten()\n",
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLeqadTFkOBl",
        "outputId": "c31b547a-af2d-4946-c094-dfbd88815ebc"
      },
      "outputs": [],
      "source": [
        "np.shape(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7x_ULqZyR_6"
      },
      "source": [
        "\n",
        "- $\\Theta = (X^T X)^{-1} X^T y$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "NBuCpOSjx4wy",
        "outputId": "1e8d9c5d-feb5-4e9f-bea0-09cb957dd91f"
      },
      "outputs": [],
      "source": [
        "plt.plot(df.X1,df.Y,\"ro\")\n",
        "x_ = np.linspace(-1, 1)\n",
        "plt.plot(x_ ,theta[0] + theta[1]*x_ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYsrWvMdHx4h"
      },
      "source": [
        "# Modelo Bidimensional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "HIjp27125qXW",
        "outputId": "fa6c8357-2c37-4e50-9762-398ae458aa2a"
      },
      "outputs": [],
      "source": [
        "N = 200\n",
        "x1 = np.linspace(-1, 1, N)\n",
        "x2 = np.linspace(-1, 1, N)\n",
        "\n",
        "X1, X2 = np.meshgrid(x1,x2)\n",
        "Y = 0.2*X1 + 0.5*X2 - 1.0\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "surf = ax.plot_surface(X1, X2, Y)\n",
        "ax.set_xlabel(\"X1\")\n",
        "ax.set_ylabel(\"X2\")\n",
        "ax.set_zlabel(\"Y1\")\n",
        "#scatter = ax.scatter(x1, x2, y,\"-\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "am5C11gkObrj"
      },
      "outputs": [],
      "source": [
        "# Ecuaciones parametricas del mismo plano:\n",
        "alpha = 2*np.random.random(N)-1\n",
        "beta  = 2*np.random.random(N)-1\n",
        "x1 = alpha\n",
        "x2 = beta\n",
        "y = 0.2*alpha + 0.5*beta - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "dAjPXlADOzga",
        "outputId": "a3818a85-8236-498e-86f3-22ac2b162027"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "surf = ax.plot_surface(X1, X2, Y, alpha=0.2)\n",
        "surf = ax.scatter(x1, x2, y, color=\"green\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6lddSc6MH1EQ"
      },
      "outputs": [],
      "source": [
        "# Regresion bi-lineal\n",
        "df = pd.DataFrame({\"Y\":y, \"X1\":x1,\"X2\":x2})\n",
        "df[\"ones\"] = np.ones(N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bPuit2V_IEfX"
      },
      "outputs": [],
      "source": [
        "y = np.reshape(df.Y.values, (N,1))\n",
        "X = df[[\"ones\",\"X1\",\"X2\"]].values\n",
        "X = np.matrix(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFFyt54JIMRu",
        "outputId": "4688c3b8-86e4-4e1c-9d2a-cda1b8e637dc"
      },
      "outputs": [],
      "source": [
        "theta = (X.T@X).I @ X.T @ y\n",
        "theta = np.array(theta).flatten()\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr5FDYckQwiS"
      },
      "source": [
        "# Datos de boston"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu1trxVSQkSP"
      },
      "outputs": [],
      "source": [
        "# Tomar los datos de las casas de boston y hacer una regresion lineal tomando\n",
        "# el average number of rooms per dwelling.\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "9Kntho4-QkSQ",
        "outputId": "6c8f0c3b-e8ef-4992-8b1e-c96362b3f860"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"mean_\":target, \"rm\":data[:,5]})\n",
        "df[\"ones\"] = np.ones(len(target))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Evc5nPSgQkSR",
        "outputId": "8a91cb07-47ea-4202-9046-7ed646982728"
      },
      "outputs": [],
      "source": [
        "plt.plot(df.rm, df.mean_,\"go\", alpha=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NEfF1XGUQkSR"
      },
      "outputs": [],
      "source": [
        "X = df[[\"ones\",\"rm\"]].values\n",
        "Y = np.reshape(df[\"mean_\"].values,(len(X),1))\n",
        "X = np.matrix(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQLs5x-NQkSS",
        "outputId": "7166e0d4-6f64-4670-c304-c77e1dee1fcf"
      },
      "outputs": [],
      "source": [
        "theta = ((X.T@X).I)@X.T@Y\n",
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "EbikbngbQkSS",
        "outputId": "8a22fcd5-4a3f-4932-d3ce-04d47044c3a6"
      },
      "outputs": [],
      "source": [
        "theta = np.array(theta).flatten()\n",
        "x = np.linspace(4, 10, 100)\n",
        "plt.figure()\n",
        "plt.plot(df.rm, df.mean_,\"go\", alpha=0.4)\n",
        "plt.plot(x,theta[0]+theta[1]*x, \"b-\")\n",
        "plt.ylabel(\"Mean\")\n",
        "plt.xlabel(\"RM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkue3DN2WxP6"
      },
      "source": [
        "# Intepretación Probabilistica.\n",
        "\n",
        "Supongamos que tenemos una caracteristica $x_i$ con m valores de entrenamiento, si asumimos que cada valor $y_i$ presenta una dispersión gaussiana $\\epsilon_i$, cada $y_i$ podrá tener el siguiente valor:\n",
        "\n",
        "$y^{i} = \\Theta^T X^{(i)} + \\epsilon_i$\n",
        "\n",
        "Asumiendo ademas que el ruido gaussiando es aleatorio y esta distribuido de forma identica, con media cero y varianza $\\sigma$, tenemos que la probabilidad de que la cantidad y tenga  dispersion $\\epsilon_i$ es:\n",
        "\\begin{equation}\n",
        "p(\\epsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( \\epsilon^{(i)}\\right)^2 }{2\\sigma ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "Escribiendo, lo anterior en terminos de la probabilidad de obtener un valor de $y^{i}$ dado un $x^{i}$ parametrizado por $\\theta$ obtenemos que:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "p_i(y^{i}|x^{i};\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( y_i - \\Theta^T X^{(i)} \\right)^2 }{2\\sigma ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Si ausmimos independicia estadística de cada $\\epsilon^{(i)}$, la probabilidad $L(\\theta)$ asociada a toda la distribución de puntos viene dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "\\cal{L}(\\theta) = p(\\vec{y}|X;\\theta)=\\prod_{i=1}^{n} p_i(y^{i}|x^{i};\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\cal{L}(\\theta) =\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( y_i - \\Theta^T X^{(i)} \\right)^2 }{2\\sigma ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "para tener la mejor estimación posible de los valores que se deben elegir de  $\\theta$, se escogeran los parámetros que generan la mayor probabilidad de ocurrencia según las observaciones, es decir, aquellos valores para el cual $L(\\theta)$ es máximo, si aplicamos el logaritmo natural antes de máximar tenemos que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\ln \\cal{L}(\\theta) = \\cal{l}(\\theta) = \\ln \\left[\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( y_i - \\Theta^T X^{(i)} \\right)^2 }{2\\sigma ^2}} \\right]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Después de un par de pasos se puede encontrar que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\cal{l}(\\theta) = n\\ln \\frac{1}{\\sqrt{2\\pi\\sigma}} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y^{i}-\\Theta^T X^{i})^2\n",
        "\\end{equation},\n",
        "\n",
        "maximar $\\cal{l(\\theta)}$ equivale a encontrar donde  $\\nabla_{\\theta} \\cal{l(\\theta)} = 0$. Lo anterior muestra por que la elección de minimos cuadrados puede ser una buena eleccción para el analisis de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intepretación Probabilistica de la regularizacion.\n",
        "\n",
        "Supongamos adicionalmente que el vector de parámetros $\\theta$ también es aleatorio, es decir es un vector de variables aleatorias, con lo cual incluiríamos entonces toda la aleatoriedad posible que pueden tener los mecanismos del sistema que estamos estudiando. La distribución de $\\theta$ es también una normal en función de la norma de $\\theta$:\n",
        "\n",
        "\\begin{equation}\n",
        "p(\\theta)=\\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{ \\left| \\theta \\right|^2 }{2 \\tau ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "Tenemos como en el caso anterior que tanto las caracteristicas x como y son variables aleatorias y que si tenemos una caracteristica $x_i$ cada valor $y_i$ presenta una dispersión gaussiana $\\epsilon_i$:\n",
        "\n",
        "$y^{i} = \\Theta^T X^{(i)} + \\epsilon_i$\n",
        "\n",
        "Asumiendo de nuevo que el ruido gaussiando es aleatorio y esta distribuido de forma identica, con media cero y varianza $\\sigma$, tenemos que la probabilidad de que la cantidad y tenga  dispersion $\\epsilon_i$ es:\n",
        "\\begin{equation}\n",
        "p(\\epsilon^{(i)})=\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( \\epsilon^{(i)}\\right)^2 }{2\\sigma ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "Ahora por la ley de Bayes podemos expresar la probabilidad de $\\theta$ dados los datos $X$ y $y$ y preguntarmos por los valores de $\\theta$ que maximizan la probabilidad.\n",
        "\n",
        "\\begin{equation}\n",
        "p(\\theta | X, y) = \\frac{p(X,y | \\theta) p(\\theta)} {p(X,Y)}\n",
        " = \\frac{P(y | X, \\theta) p(\\theta)}{p(Y|X)}\n",
        "\\end{equation}\n",
        "donde asumimos la independencia de $X$ de $\\theta$ con la propiedad $p(X|\\theta)=p(X)$\n",
        "\n",
        "El máximo de la probabilidad se obtiene maximizando el nominador pues el denominador no depende de $\\theta$. Estos parámetros que se obtienen por este método se denominan Maximum a Posteriori MAP. Note que la expresión es la misma anterior excepto que ahora está multiplicada por la probabilidad de $\\theta$\n",
        "\n",
        "Igualmente tenemos la probabilidad de obtener un valor de $y^{i}$ dado un $x^{i}$ y un $\\theta$ dados por:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "p_i(y^{i}|x^{i},\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ \\left( y_i - \\Theta^T X^{(i)} \\right)^2 }{2\\sigma ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "La función a maximiar está dada por\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{MAP} = \\arg \\max_{\\theta} p(\\vec{y}|X,\\theta) p(\\theta) \n",
        "= \\arg \\max_{\\theta} \\prod_{i=1}^{n} p_i(y^{i}|x^{i},\\theta) p(\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "Tomando el logaritmo de la función a maximizar\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{MAP} = \\arg \\max_{\\theta}  \\sum_ {i=1}^{n} \\log  \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{ ( y_i - \\Theta^T X^{(i)} )^2 }{2\\sigma ^2}}  + \\log \\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{ \\left| \\theta \\right|^2 }{2 \\tau ^2}}\n",
        "\\end{equation}\n",
        "\n",
        "y siguiendo la derivación anterior se obtiene el mismo coste de los mínimos cuadrados con un termino adicional:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{MAP} = \\arg \\min_{\\theta} J(\\theta) + \\frac{\\sigma^2}{\\tau^2} \\left| \\theta \\right|^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Este término se llama regularización y depende del parámetro que acompaña la dispersión de $\\theta$. El método de mínimos cuadrados se obtiene con esa dispersión tendiendo a infinito mientras que toma mas importancia a medida que esa dispersión se hace pequeña.\n",
        "\n",
        "Otra manera de aproximar ese termino de regularización es ver como se minimiza para un valor finito. La norma de $\\theta$ se minimiza haciendo los valores de $\\theta$ lo mas pequeños posibles, posiblemente anulando algunos de estos. Es por esto que este termino va a ser responsable de reducir la complejidad, reduciendo el número de parámetros del modelo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
