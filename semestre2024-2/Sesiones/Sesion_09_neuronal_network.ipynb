{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2024-2/Sesiones/Sesion_09_neuronal_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhD4V3tMzm9n"
   },
   "source": [
    "# Redes neuronales, Teoría\n",
    "\n",
    "### Interpretación de la regresión logística en terminos de redes neuronales\n",
    "\n",
    "Consideremos una neurona con  $3$ valores de entrada $x_1$, $x_2$ y  $x_3$ asociados a tres caracteristicas,  como se muestra en la figura:\n",
    "\n",
    "<img src=\"https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/imagenes/nn_fig0.png?raw=true\" width=\"200\">\n",
    "\n",
    "La neurona tiene como entradas los parametros $\\theta_1,\\theta_2$ y  $\\theta_3$\n",
    "que permite realizar la optimización de la red neuronal, asi mismo, existe un parametro $b$ conocido como el bias que permite desplazar o subir el hyperplano para el modelo que se desea ajustar. De forma similar a la regresión logistica, podemos definir un hyperplano para los parameros considerados de la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + b = \\theta^{T} X\n",
    "\\end{equation}\n",
    "\n",
    "$\\theta=[b, \\theta_1, \\theta_2, \\theta_3,..., \\theta_n ]$\n",
    "\n",
    "Para un problema de clasificación podemos elegir diferentes funciones de probabildiad que permitan definir si una clase 1 o 0 pertenece o no pertenece al conjunto, para ello podemos definir diferentes distribuciones de probabilidad que garantice la clasificación. Sea  f la función que permite hacerlo, asi si, $f>0.5$ entonces tenemos la clase tipo 1, y en caso contrario la clase tipo 0.\n",
    "\n",
    "De esta forma la salida de esta red neuronal es:  \n",
    "\n",
    "\\begin{equation}\n",
    "  f(X, \\theta, b) = f (z) = a$.\n",
    "\\end{equation}\n",
    "\n",
    "La función $f(z)$  es conocida como funcion de activación, algunas de estas funciones que pueden ser usadas son dadas a continuación:\n",
    "  \n",
    "\n",
    "-  Sigmoide :\n",
    "\\begin{equation}\n",
    "f(x)=\\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "- $\\mathrm {tanh(x)}$\n",
    "\n",
    "\n",
    "- Gaussian :\n",
    "\\begin{equation}\n",
    "f(x)=Ae^{-bx}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "La función de activación sigmoide cumple la siguiente condición de que $f'(x)= f(x)(1-f(x))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQIJEpdjJWkr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM74wLbd3knq"
   },
   "source": [
    "![activacion](https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/imagenes/activacion.png?raw=true)\n",
    "\n",
    "\n",
    "Las ecuaciones anterior puede ser generalizada para un sistema de n-entradas.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/imagenes/nn_fig1.png?raw=true\" width=\"200\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\theta^{T} X\n",
    "\\end{equation}\n",
    "\n",
    "$\\theta=[b, \\theta_1, \\theta_2, \\theta_3,..., \\theta_n ]$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxml4iRu7lem"
   },
   "source": [
    "# Generalización de salidas de una red neuronal con L capas y $n^{l}$ neuronas por capas$\n",
    "\n",
    "Una red neuronal puede ser construida para clasificar determinado tipode conjuntos, así, la arquitectura de la  red neuronal debe ser tal que capture características simples hasta complejas, la primera capa podría estar asociada a determinar algunos patrones simples, después de detectar esos patrones simples se puede emplear el resultado para detectar estructuras más complejas, en las siguientes capas.\n",
    "\n",
    "Una generalización de la red puede ser intuida como sigue:\n",
    "\n",
    "Supongamos que tenemos la siguiente red neuronal con L+1 capas, las capas 0 y la capa L, son las capas de entrada y salida de la red neuronal respectivamente . Las otras L-1 se conocen como capas ocultas.\n",
    "\n",
    "<img src=\"https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/imagenes/nn_fig2.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "Para la imagen se observa que la capa $l=0, l=1, l=2, ..., l=k,..., l=L-1, l=L $ tendran  $n^{[l]}$ neruronas :\n",
    "\n",
    "$n^{[0]} = 3, n^{[1]} = 4, n^{[2]} = 4,..., n^{[l]} = k, ..., n^{[L-1]} = p, n^{[L]} = 1$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlvYNDwjyrAY"
   },
   "source": [
    "\n",
    "- ## Capa $n^{[0]}=3$\n",
    "\n",
    "Dada la imagen se puede deducir que tenemos 3 caracteristicas $\\vec{X}^{[0](0)}$ cuyas salidas las representareomos por $\\vec{a}^{[0](0)}$. Conjunto de caracteristicas en la capa 0 en el entrenamiento 0\n",
    "\n",
    "\n",
    "$\\vec{X}^{[0](0)} = (x_{0}^{[0](0)},x_{1}^{[0](0)},x_{2}^{[0](2)})=(a_{0}^{[0](0)},a_{1}^{[0](0)},a_{2}^{[0](2)})= \\vec{A}^{[0](0)}$\n",
    "\n",
    "\n",
    "\n",
    "-  ## Capa $n^{[1]}=4$\n",
    "\n",
    "Para cada neurona se calcula los hyperplanos ($Z$) dependiendo del número de parametros que tiene la capa  anterior, y se calcula la función de probabilidad (f) para obtener unas nueva salida  $A$.\n",
    "\n",
    "\n",
    "Sea $Z_i^{[l](m)}$: hyperplano de la neurona i-esima en la capa l y el entrenamiento m\n",
    "\n",
    "\n",
    "\n",
    "$z_1^{[1](0)} = \\theta_{11}^{[1](0)} a_1 +  \\theta_{12}^{[1](0)} a_2+ \\theta_{13}^{[1](0)} a_3  b_{11}^{[1](0)}$\n",
    "\n",
    "\n",
    "$z_2^{[1](0)} = \\theta_{11}^{[1](0)} a_1 +  \\theta_{12}^{[1](0)}a_2 + \\theta_{13}^{[1](0)} a_3+ b_{11}^{[1](0)}$\n",
    "\n",
    "\n",
    "$z_3^{[1](0)} = \\theta_{11}^{[1](0)}a_1 +  \\theta_{12}^{[1](0)}a_2 + \\theta_{13}^{[1](0)}a_3  + b_{11}^{[1](0)}$\n",
    "\n",
    "\n",
    "$z_4^{[1](0)} = \\theta_{11}^{[1](0)}a_1 +  \\theta_{12}^{[1](0)}a_2 + \\theta_{13}^{[1](0)}a_3  + b_{11}^{[1](0)}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En forma matricial tenemos que :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4\n",
    "\\end{bmatrix}^{[1](0)}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & \\theta_{13}\\\\\n",
    "\\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n",
    "\\theta_{31} & \\theta_{32} & \\theta_{33} \\\\\n",
    "\\theta_{41} & \\theta_{42} & \\theta_{43} \\\\\n",
    "\\end{bmatrix}^{[1]} _{4\\times 3}\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3 \\\\\n",
    "a_4\n",
    "\\end{bmatrix}^{[0](0)} +\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4\n",
    "\\end{bmatrix}^{[1]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{Z}^{[1](0)} = \\Theta^{[1]} \\vec{A}^{[0](0)} + \\vec{b}^{[1]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Aplicando la función probabilidad tenemos que:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\vec{A}^{[1](0)} = f(\\vec{Z}^{[1]}) =f( \\Theta^{[1]} \\vec{A}^{[0](0)} + \\vec{b}^{[1]})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9dfch4mD4xc"
   },
   "source": [
    "-  ## Capa $n^{[2]} =4$\n",
    "\n",
    "\n",
    "\n",
    "En forma matricial tenemos que :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4\n",
    "\\end{bmatrix}^{[2](0)}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & \\theta_{13} & \\theta_{14}\\\\\n",
    "\\theta_{21} & \\theta_{22} & \\theta_{23} & \\theta_{24} \\\\\n",
    "\\theta_{31} & \\theta_{32} & \\theta_{33} & \\theta_{34} \\\\\n",
    "\\theta_{41} & \\theta_{42} & \\theta_{43} & \\theta_{44}\\\\\n",
    "\\end{bmatrix}^{[2]} _{4\\times 4}\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3 \\\\\n",
    "a_4\n",
    "\\end{bmatrix}^{[1]} +\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4\n",
    "\\end{bmatrix}^{[2]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{Z}^{[2](0)} = \\Theta^{[2]} \\vec{A}^{[1](0)} + \\vec{b}^{[2]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Aplicando la función probabilidad tenemos que:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\vec{A}^{[2](0)} = f(\\vec{Z}^{[2](0)}) =f( \\Theta^{[2]} \\vec{A}^{[1](0)} + \\vec{b}^{[2]})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4fhAmJqHqFt"
   },
   "source": [
    "-  ## Capa $n^{[l]} = k$\n",
    "\n",
    "\n",
    "\n",
    "En forma matricial tenemos que :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{n^{[l]}}\n",
    "\\end{bmatrix}^{[l](0)}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & . & .& .& \\theta_{1n^{[l-1]}}\\\\\n",
    "\\theta_{21} & \\theta_{22} & . & .& .& \\theta_{2n^{[l-1]}}\\\\\n",
    ". & .  & . &   & & .\\\\\n",
    ". & .  &   & . & & .\\\\\n",
    ". & .  &   &  & .& .\\\\\n",
    "\\theta_{n^{[l]}1} & \\theta_{n^{[l]}2} & . & .& .& \\theta_{n^{[l]}n^{[l-1]}}\\\\\n",
    "\\end{bmatrix}^{[l]}_{n^{[l]} \\times n^{[l-1]}}\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{n^{[l-1]}}\\\\\n",
    "\\end{bmatrix}^{[L-1](0)}+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "b_{n^{[l]}}\\\\\n",
    "\\end{bmatrix}^{[L]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "De forma general,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{Z}^{[l](0)} = \\Theta^{[l]} \\vec{A}^{[l-1](0)} + \\vec{b}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Aplicando la función probabilidad tenemos que:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\vec{A}^{[l](0)} = f(\\vec{Z}^{[l](0)}) =f( \\Theta^{[l]} \\vec{A}^{[l-1](0)} + \\vec{b}^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Notese que:\n",
    "\n",
    "\n",
    "$\\mathrm{dim(\\vec{Z}^{[l]})}=n^{[l]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{A}^{[l]})}=n^{[l-1]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finalmente para la capa L-esima tenemos que:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "\\end{bmatrix}^{[L](0)}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & . & .& .& \\theta_{1n^{[L-1]}}\\\\\n",
    "\\end{bmatrix}^{[L]}_{1 \\times n^{[L-1]}}\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "a_{n^{[L-1]}}\\\\\n",
    "\\end{bmatrix}^{[l-1](0)}+\n",
    "\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "\\end{bmatrix}^{[l]}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5MDXgEARB1-"
   },
   "source": [
    "# Con m datos de entrenamientos.\n",
    "\n",
    "Para $m$ datos de entrenamiento, las expresión anteriores pueden ser resumidas en las siguientes ecuaciones\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1^{(0)}  &z_1^{(1)} & .&.& .&z_1^{(m)}\\\\\n",
    "z_2^{(0)}  &z_2^{(1)} &. &.&  .&z_2^{(m)}\\\\\n",
    ".          & .        &. & &   &.      \\\\\n",
    ".          & .        &  &. &   &.      \\\\\n",
    ".          & .        &  &  & .&      \\\\\n",
    "z_{n^{[l]}}^{(0)}&z_{n^{[l]}}^{(1)} & . & .& .& z_{n^{[l]}}^{(m)}        \\\\\n",
    "\\end{bmatrix}^{[l]}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & . & .& .& \\theta_{1n^{[l-1]}}\\\\\n",
    "\\theta_{21} & \\theta_{22} & . & .& .& \\theta_{2n^{[l-1]}}\\\\\n",
    ". & .  & . &   & & .\\\\\n",
    ". & .  &   & . & & .\\\\\n",
    ". & .  &   &  & .& .\\\\\n",
    "\\theta_{n^{[l]}1} & \\theta_{n^{[l]}2} & . & .& .& \\theta_{n^{[l]}n^{[l-1]}}\\\\\n",
    "\\end{bmatrix}^{[l]}_{n^{[l]} \\times n^{[l-1]}}\n",
    "\\begin{bmatrix}\n",
    "a_1^{(0)}  &a_1^{(1)} & .&.& .&a_1^{(m)}\\\\\n",
    "a_2^{(0)}  &a_2^{(1)} &. &.&  .&a_2^{(m)}\\\\\n",
    ".          & .        &. & &   &.      \\\\\n",
    ".          & .        &  &. &   &.      \\\\\n",
    ".          & .        &  &  & .&      \\\\\n",
    "a_{n^{[L-1]}}^{(0)}&a_{n^{[L-1]}}^{(1)} & . & .& .& a_{n^{[L-1]}}^{(m)}        \\\\\n",
    "\\end{bmatrix}^{[l-1]} +\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "b_{n^{[l]}}\\\\\n",
    "\\end{bmatrix}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Escrito de una formas mas compacta tenemos que:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "[ \\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ]= \\Theta^{[l]} [\\vec{A}^{[l-1](0)},\\vec{A}^{[l-1](1)},...,\\vec{A}^{[l-1](m)} ]+ \\vec{b}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "Aplicando la funcion de activación:\n",
    "\n",
    "\\begin{equation}\n",
    "[\\vec{A}^{[l](0)},\\vec{A}^{[l](1)},...,\\vec{A}^{[l](m)} ]=f([\\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ])\n",
    "\\end{equation}\n",
    "\n",
    "Las dimensiones de las expresiones anteriores, pueden ser resumidas en lo siguiente:\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mptkEBmjJX9M"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQUiENlpSNj8"
   },
   "source": [
    "# Gradiente descendente.\n",
    "\n",
    "La salida de la última capa de entrenamiento, puede ser escrita como sigue:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\cal{A}}^{[L]} = f(\\vec{\\cal{Z}}^{[L]} )= f({\\Theta}^{[L]}\\vec{\\cal{A}}^{[L-1]} )\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "[\\vec{A}^{[L](0)},\\vec{A}^{[L](1)},...,\\vec{A}^{[L](m)} ]=f([\\vec{Z}^{[L](0)},\\vec{Z}^{[L](1)},...,\\vec{Z}^{[L](m)}  ])\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Realizando la optimización para todos los valores de $\\Theta$ tenemos que:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Theta = \\Theta -\\alpha \\nabla  J\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "con\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\Theta) =\\frac{1}{m}\\sum_i{\\cal L^{(i)}}(Y^{(i)},{\\cal A}^{(i)[L]}) = \\frac{1}{m}\\sum_{i=0}^{m-1}  [Y^{(i)} \\log( {\\cal A} ^{(i)[L]}) - (1-Y^{(i)})\\log(1-{\\cal A}^{(i)[L]})]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Calculemos las derivadas:\n",
    "- \\begin{equation}\n",
    "d\\Theta^{[L]}=\\frac{\\partial J}{\\partial \\theta_{ij}} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial {\\cal L^{(i)}}}{\\partial {\\cal A}^{(i)[L]}} \\frac{\\partial  {\\cal A}^{(i)[L]} }{\\partial {\\cal Z}^{(i)[L]}}  \\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- \\begin{equation}\n",
    "db^{[L]}=\\frac{\\partial J}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial {\\cal L^{(i)}}}{\\partial {\\cal A}^{(i)[L]}} \\frac{\\partial  {\\cal A}^{(i)[L]} }{\\partial {\\cal Z}^{(i)[L]}}  \\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial b}\n",
    "\\end{equation}\n",
    "\n",
    "Calculando cada unas de las derivadas:\n",
    "\n",
    "\\begin{equation}\n",
    "dA^{[L]}=\\frac{\\partial {\\cal L^{(i)} }}{\\partial {\\cal A}^{(i)[L]}} =    \\left[ \\frac{Y^{(i)}}{ {\\cal A} ^{(i)[L]}} - \\frac{(1-Y^{(i)})}{1-{\\cal A}^{(i)[L]}} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Empledo el hecho de que en la capa L-ésima se tiene un regresión logística y se cumple que f'=f(1-f), equivalente a tener la salida A(Ver primera ecuación de esta sección):\n",
    "\\begin{equation}\n",
    "\\frac{\\partial  {\\cal A}^{(i)[L]} }{\\partial {\\cal Z}^{(i)[L]}}   = A({\\cal Z}^{(i)[L]})(1-A({\\cal Z}^{(i)[L]})) = f'({\\cal Z}^{(i)[L]})\n",
    "\\end{equation}\n",
    "\n",
    "Teniendo presente que (Ver demostracion en el apendice):\n",
    "\n",
    "- \\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}} = A^{(i)[L-1]}\n",
    "\\end{equation}\n",
    "\n",
    "- \\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial b} = \\vec{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "La derivada de la capa $L$-esima viene dada por:\n",
    "\n",
    "- \\begin{equation}\n",
    "d\\Theta^{(i)[L]} =  dA^{(i)[L]} f'({\\cal Z}^{(i)[L]}) A^{(i)[L-1]} = d{\\cal Z}^{(i)[L]} A^{(i)[L-1]}\n",
    "\\end{equation}\n",
    "\n",
    "- \\begin{equation}\n",
    "db^{(i)[L]} =  dA^{(i)[L]} f'({\\cal Z}^{(i)[L]}) A^{(i)[L-1]} =  d{\\cal Z}^{(i)[L]}  \n",
    "\\end{equation}\n",
    "\n",
    "donde $d{\\cal Z}^{(i)[L]} = dA^{(i)[L]} f'({\\cal Z}^{(i)[L]}) $\n",
    "\n",
    "Para una capa $l$ arbitraria tenemos que:\n",
    "\n",
    "- \\begin{equation}\n",
    "d\\Theta^{[l]} =  d{\\cal Z}^{(i)[l]} A^{(i)[l-1]} = dA^{(i)[l]} f'({\\cal Z}^{(i)[l]} ) A^{(i)[l-1]}\n",
    "\\end{equation}\n",
    "\n",
    "- \\begin{equation}\n",
    "db^{[l]} =  d{\\cal Z}^{(i)[L]}  = dA^{(i)[l]} f'({\\cal Z}^{(i)[l]} )\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Los valores de dA pueden ser escritos como:\n",
    "- \\begin{equation}\n",
    "dA^{(i)[l-1]} = \\Theta^{l} dZ^{(i)[l]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "En forma vectorial tenemos que:\n",
    "\n",
    "\n",
    "- $ dZ^{[l]} = dA * f'^{[l]} (Z^{[l]})$\n",
    "\n",
    "- $ d\\Theta^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial \\Theta^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{1}$\n",
    "- $ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{2}$\n",
    "- $ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = \\theta^{[l] T} dZ^{[l]} \\tag{3}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOF2fCDFH5DV"
   },
   "source": [
    "# Algoritmo General\n",
    "\n",
    "1. Creación de la arquitectura de la red neuronal.\n",
    "2. Inicialización de parámetros\n",
    "3. Aplicar el forward pass para toda la red\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "[ \\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ]= \\Theta^{[l]} [\\vec{A}^{[l-1](0)},\\vec{A}^{[l-1](1)},...,\\vec{A}^{[l-1](m)} ]+ \\vec{b}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "Aplicando la funcion de activación:\n",
    "\n",
    "\\begin{equation}\n",
    "[\\vec{A}^{[l](0)},\\vec{A}^{[l](1)},...,\\vec{A}^{[l](m)} ]=f([\\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ])\n",
    "\\end{equation}\n",
    "\n",
    "Las dimensiones de las expresiones anteriores, pueden ser resumidas en lo siguiente:\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
    "\n",
    "4. Calcular la funcion de coste\n",
    "\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "\n",
    "\n",
    "5. Aplicar el Backwardpass\n",
    "\\begin{equation}\n",
    "dZ^{[l]} = dA^{[l]} * f'^{[l]} (Z^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "$ d\\Theta^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial \\Theta^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{1}$\n",
    "$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{2}$\n",
    "$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = \\theta^{[l] T} dZ^{[l]} \\tag{3}$\n",
    "\n",
    "6. Calcular el gradiente descendente, actualizar parametros W.\n",
    "\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "7. Repetir desde el paso 3, terminar cuando J sea minimizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtiEQnTSWrJ5"
   },
   "source": [
    "# Apendice 1.\n",
    "\n",
    "Demostracion de la derivadas\n",
    "\n",
    "\\begin{equation}\n",
    "dJ = \\frac{\\partial J}{\\partial \\theta_{ij}} =  \\frac{1}{m}\\sum_{i=0}^{m-1}  \\left[ \\frac{Y^{(i)}}{ {\\cal A} ^{(i)[L]}} - \\frac{(1-Y^{(i)})}{1-{\\cal A}^{(i)[L]}} \\right] {\\cal A}^{(i)[L]} (1-{\\cal A}^{(i)[L]})  \\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "simplificando, basado en lo siguiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \\frac{y}{a}-\\frac{(1-y)}{1-a}\\right)a(1-a)=y(1-a)-(1-y)a=y-a\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\theta_{ij}} =  \\frac{1}{m}\\sum_{i=0}^{m-1}  \\left[ Y^{(i)} - {\\cal A}^{(i)[L]} \\right] \\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial \\theta_{ij}} =  \\frac{1}{m}\\sum_{i=0}^{m-1}  \\left[ Y^{(i)} - {\\cal A}^{(i)[L]} \\right] \\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En forma matricial tenemos que:\n",
    "\n",
    "Sea $A_r^{(j)[L-1]}$, la componente r-esima en el entrenamiento j-esimo en la capa $(L-1)$, sea $\\Theta_{ir}$ la matriz de parametros que llegan a la neurona r en la capa L, provenientes de las neurona i-esima en la capa L-1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}} = \\frac{\\partial }{\\partial \\theta_{ij}} \\sum_   {r=1}^{n^{[L-1]}} \\Theta_{ir} A_{r}^{(j)[L-1]} =  \\sum_{r=1}^{n^{[L-1]}} \\delta_{ir} A_{r}^{(j)[L-1]} + \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir} \\frac{\\partial\n",
    " A_{r}^{(j)[L-1]}}{\\partial \\theta_{ij}} = A_{i}^{(j)[L-1]}+ \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-1]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "Empleando el hecho de que :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal A}^{(i)[L-1]}}{\\partial \\theta_{ij}}= \\frac{\\partial f({\\cal Z}^{(i)[L-1]}) }{\\partial {\\cal Z}^{(i)[L-1]}}  \\frac{\\partial {\\cal Z}^{(i)[L-1]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "y suponiendo que en las capas anteriores a la L esima tenemos  función de activacion diferente a la logistica, encontramos lo siguiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal A}^{(i)[L-1]}}{\\partial \\theta_{ij}}=  f'({\\cal Z}^{(i)[L-1]})\\frac{\\partial {\\cal Z}^{(i)[L-1]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal A}^{(j)[L-1]}}{\\partial \\theta_{ij}}=  f'({\\cal Z}^{(j)[L-1]}) \\left( A_{i}^{(j)[L-2]} + \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-2]}}{\\partial \\theta_{ij}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "(Revisar termino entre parentesis de la expresión anterior y or ende los resultados que siguen)\n",
    "\n",
    "Asi, obtenemos lo siguiente:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(j)[L]}}{\\partial \\theta_{ij}} = A_{i}^{(j)[L-1]}+ \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} +  \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-2]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal A}^{(i)[L-2]}}{\\partial \\theta_{ij}}=  f'({\\cal Z}^{(i)[L-2]})\\frac{\\partial {\\cal Z}^{(i)[L-2]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal A}^{(j)[L-2]}}{\\partial \\theta_{ij}}=  f'({\\cal Z}^{(j)[L-2]}) \\left( A_{i}^{(j)[L-3]} + \\sum_{r=1}^{n^{[L-3]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-3]}}{\\partial \\theta_{ij}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(j)[L]}}{\\partial \\theta_{ij}} = A_{i}^{(j)[L-1]}+ \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} +  \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir}\n",
    " f'({\\cal Z}^{(j)[L-2]}) A_{i}^{(j)[L-3]} + \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir}\n",
    " f'({\\cal Z}^{(j)[L-2]})\\sum_{r=1}^{n^{[L-3]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-3]}}{\\partial \\theta_{ij}} + ...+\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(j)[L]}}{\\partial \\theta_{ij}} = A_{i}^{(j)[L-1]}+ \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} +  \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir}\n",
    " f'({\\cal Z}^{(j)[L-2]}) A_{i}^{(j)[L-3]} + \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir}\n",
    " f'({\\cal Z}^{(j)[L-2]})A_{i}^{(j)[L-3]} + \\sum_{r=1}^{n^{[L-1]}} \\Theta_{ir}  f'({\\cal Z}^{(j)[L-1]}) A_{i}^{(j)[L-2]} \\sum_{r=1}^{n^{[L-2]}} \\Theta_{ir}\n",
    " f'({\\cal Z}^{(j)[L-2]})A_{i}^{(j)[L-3]} \\sum_{r=1}^{n^{[L-4]}} \\Theta_{ir} \\frac{\\partial A_{r}^{(j)[L-4]}}{\\partial \\theta_{ij}} + ...+\\\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Notese que las expresiones de arriba, en cada cada capa, generan la derivadas implicita que se debe emplear bajo esta metodología\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVID2UcdWtz9"
   },
   "source": [
    "\n",
    "#Apendice 2\n",
    "\n",
    "Calculos en forma matricial para el  hyperplanos L:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}=\\frac{\\partial }{\\partial \\theta_{ij}}\\begin{bmatrix}\n",
    "\\theta_{11}a_1^{(0)[L-1]} + .  . . +\\theta_{1n^{[l-1]}} a_{n^{[l-1]}}^{(0)[L-1]}&.&.&.&\\theta_{11}a_1^{(m)[L-1]} + .  . . +\\theta_{1n^{[l-1]}} a_{n^{[l-1]}}^{(m)[L-1]}\\\\\n",
    "\\theta_{21}a_1^{(0)[L-1]} +. . .+ \\theta_{2n^{[l-1]}} a_{n^{[l-1]}}^{(m)[L-1]}&.&.&.&\\theta_{21}a_1^{(m)[L-1]} +. . .+ \\theta_{2n^{[l-1]}} a_{n^{[l-1]}}^{(m)[L-1]}\\\\\n",
    ". &.& & &\\\\\n",
    ". & &.& &\\\\\n",
    ". & & &.&\\\\\n",
    "\\theta_{n^{[l]}1}a_1^{(0)[L-1]} +. . . +\\theta_{n^{[l]}n^{[l-1]}} a_{n^{[l-1]}}^{(m)[L-1]}&.&.&.&\\theta_{n^{[l]}1}a_1^{(m)[L-1]} +. . . +\\theta_{n^{[l]}n^{[l-1]}} a_{n^{[l-1]}}^{(m)[L-1]}\\\\\n",
    "\\end{bmatrix}_{n^{[l]} \\times m}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reescribiendo solo el termion ij-esimo tenemos que:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}=\\frac{\\partial }{\\partial \\theta_{ij}}\\begin{bmatrix}\n",
    "...\\theta_{1k}a_k^{(0)[L-1]}...&.&.&.& ...\\theta_{1k}a_k^{(m)[L-1]}.... \\\\\n",
    "...\\theta_{2k}a_k^{(0)[L-1]}...&.&.&.& ...\\theta_{2k}a_{k}^{(m)[L-1}...\\\\\n",
    ". &.& & &\\\\\n",
    ". & &.& &\\\\\n",
    ". & & &.&\\\\\n",
    "...\\theta_{n^{[l]}k}a_k^{(0)[L-1]}... &.&.&.&...\\theta_{n^{[l]}k}a_k^{(m)[L-1]}...\\\\\n",
    "\\end{bmatrix}_{n^{[l]} \\times m}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial {\\cal Z}^{(i)[L]}}{\\partial \\theta_{ij}}=\\begin{bmatrix}\n",
    "a_k^{(0)[L-1]}\\delta_{kj}&.&.&.& a_k^{(m)[L-1]}\\delta_{kj} \\\\\n",
    "a_k^{(0)[L-1]}\\delta_{kj}&.&.&.& a_{k}^{(m)[L-1}\\delta_{kj}\\\\\n",
    ". &.& & &\\\\\n",
    ". & &.& &\\\\\n",
    ". & & &.&\\\\\n",
    "a_k^{(0)[L-1]}\\delta_{kj} &.&.&.&a_k^{(m)[L-1]}\\delta_{kj}\\\\\n",
    "\\end{bmatrix}_{n^{[l]} \\times m} + \\Theta_{ij}\\frac{\\partial A^{[L-1]}}{\\partial \\theta_{ij}}= [\\vec{A}^{[L](0)},\\vec{A}^{[L](1)},...,\\vec{A}^{[L](m)} ] +  \\Theta_{ij}\\frac{\\partial A^{[L-1]}}{\\partial \\theta_{ij}}=  {\\cal A}^{[L-1]} +  \\Theta_{ij}\\frac{\\partial A^{[L-1]}}{\\partial \\theta_{ij}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szVl3luOW0Vr"
   },
   "source": [
    "# Referencias de interes\n",
    "https://quickdraw.withgoogle.com/\n",
    "\n",
    "https://cloud.google.com/bigquery-ml/docs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
